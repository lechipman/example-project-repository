{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b5f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import directories\n",
    "import os\n",
    "import pathlib\n",
    "import zipfile\n",
    "\n",
    "import contextily as cx\n",
    "import io\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84687e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "working_dir = os.path.join(\n",
    "    pathlib.Path.home(), 'earth-analytics', 'data', 'watershed-project')\n",
    "if not os.path.exists(working_dir):\n",
    "    print('{} does not exist. Creating...'.format(working_dir))\n",
    "    os.makedirs(working_dir)\n",
    "\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e4f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define urls for plot data downloads\n",
    "# Site coordinates (saved on github)\n",
    "sites_url = (\"https://raw.githubusercontent.com/lechipman/\"\n",
    "             \"watershed-project/master/UAV_gps_coords.csv\")\n",
    "# Watershed boundary, USGS \n",
    "wbd_10_url = (\n",
    "    \"https://prd-tnm.s3.amazonaws.com/StagedProducts/\"\n",
    "    \"Hydrography/WBD/HU2/Shape/WBD_10_HU2_Shape.zip\")\n",
    "\n",
    "# Boulder County streams data (University of Colorado, Boulder, GeoLibrary)\n",
    "# https://geo.colorado.edu/catalog/47540-5ca23860d43267000b8c744e\n",
    "stream_url = (\"https://geo.colorado.edu/apps/geolibrary/\"\n",
    "              \"datasets/STREAMSx4.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583a26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the site coordinates for plotting (saved on github)\n",
    "sites_download = requests.get(sites_url).content\n",
    "\n",
    "# Read the downloaded content as a pandas dataframe\n",
    "sites_df = pd.read_csv(io.StringIO(sites_download.decode('utf-8')))\n",
    "\n",
    "# Select one location from each site to map\n",
    "sites_short_df = sites_df.iloc[[0, 7, 17, 29, -1]]\n",
    "\n",
    "# Create gdf of study sites\n",
    "sites_gdf = gpd.GeoDataFrame(\n",
    "    sites_short_df,\n",
    "    geometry=gpd.points_from_xy(sites_short_df['lon'],\n",
    "                                sites_short_df['lat']),\n",
    "    crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104e9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download data and unzip files\n",
    "def download_data(data_url, data_name):\n",
    "    \"\"\"Downloads Data to a Local Directory\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_url: str\n",
    "        Url to the desired data.\n",
    "    data_name: str\n",
    "        The name of the data.\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        A geodataframe of requested data.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    override_cache = False\n",
    "    data_dir = data_name\n",
    "    data_path = (os.path.join(data_dir, data_dir + '.zip'))\n",
    "    \n",
    "    # Cache data file\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "        if (not os.path.exists(data_path)) or override_cache:\n",
    "            print('{} does not exist. Downloading...'.format(data_path))\n",
    "            # Download full data file as zipfile\n",
    "            response = requests.get(data_url)\n",
    "\n",
    "            # Write in respose content using context manager\n",
    "            with open(data_path, 'wb') as data_file:\n",
    "                data_file.write(response.content)\n",
    "\n",
    "            # Decompress zip file\n",
    "            with zipfile.ZipFile(data_path, 'r') as data_zipfile:\n",
    "                data_zipfile.extractall(data_dir)\n",
    "    \n",
    "    # For special case where data is downloaded in subfolders (WDB)\n",
    "    # define new path to data and load as gdf\n",
    "    if (data_name == 'water-boundary-dataset-hu10'):\n",
    "        new_data_path = os.path.join(data_dir, 'Shape', 'WBDHU8.shp')\n",
    "        temp_gdf = gpd.read_file(new_data_path)\n",
    "        gdf = temp_gdf[temp_gdf.name.str.contains('Vrain')]\n",
    "    \n",
    "     # Otherwise load data from original path as gdf\n",
    "    else:        \n",
    "        gdf = gpd.read_file(data_path)\n",
    "\n",
    "    # Set CRS of gdf to same as site points\n",
    "    crs_gdf = gdf.to_crs(crs='EPSG:4326')\n",
    "             \n",
    "    return crs_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2abc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gdf of st. vrain watershed boundary dataset\n",
    "vrain_gdf = download_data(data_url = wbd_10_url, \n",
    "                        data_name = 'water-boundary-dataset-hu10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6055fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gdf of Colorado streams\n",
    "stream_gdf = download_data(data_url = stream_url, \n",
    "                        data_name = 'co_streams')\n",
    "\n",
    "# Clip stream data to st vrain watershed boundary\n",
    "stream_clipped_gdf = stream_gdf.clip(vrain_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e414b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionaries for mapping\n",
    "site_symbol_dict = {\n",
    "        'AV GCP1': '*',\n",
    "        'HW93 GCP1': '*',\n",
    "        'LEG1-GCP1': '*',\n",
    "        'VV GCP1': '*',\n",
    "        'HM': '*'\n",
    "    }\n",
    "\n",
    "site_name_dict = {\n",
    "        'AV GCP1': 'Apple Valley North',\n",
    "        'HW93 GCP1': 'Highway 93',\n",
    "        'LEG1-GCP1': 'Legacy 1',\n",
    "        'VV GCP1': 'Van Vleet',\n",
    "        'HM': 'Hall Meadows'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03550224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Watershed and Streams - Method 1 (matplotlib)\n",
    "def plot_sites():\n",
    "    \"\"\"Creates a map of study sites in the St. Vrain Watershed\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 16))\n",
    "    ax.set_title(\"Site Locations in the St. Vrain Watershed\",\n",
    "                 pad=20,\n",
    "                 fontsize=16)\n",
    "\n",
    "    stream_clipped_gdf.plot(ax=ax, color='blue')\n",
    "    vrain_gdf.plot(ax=ax, facecolor='cyan', alpha=0.5)\n",
    "\n",
    "    for i, gdf in sites_gdf.groupby('name'):\n",
    "        gdf.plot(ax=ax,\n",
    "                 marker=site_symbol_dict[i],\n",
    "                 label=site_name_dict[i],\n",
    "                 markersize=150,\n",
    "                 legend=True,\n",
    "                 zorder=3)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0)\n",
    "    cx.add_basemap(ax, crs=vrain_gdf.crs, zoom=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ea9a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Watershed and Streams - Method 2 (folium)\n",
    "def plot_sites_folium():\n",
    "    \"\"\"Creates a map of study sites in the St. Vrain Watershed\"\"\"\n",
    "    \n",
    "    # Create map centered around Boulder\n",
    "    m = folium.Map(\n",
    "        location=[40.0150, -105.2705],\n",
    "        tiles=\"Stamen Terrain\",\n",
    "        zoom_start=11\n",
    "    )\n",
    "\n",
    "    folium.GeoJson(\n",
    "        vrain_gdf, \n",
    "        name=\"St. Vrain Watershed\").add_to(m)\n",
    "\n",
    "    for index, row in sites_short_df.groupby('name'):\n",
    "        folium.Marker(\n",
    "            location=[row.lat, row.lon],\n",
    "            popup=site_name_dict[index]\n",
    "        ).add_to(m)\n",
    "        \n",
    "    return m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
